{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the relevant Python libraries that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d87f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4357a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the data set and evaluate the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daeb17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/churn_raw_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6006623",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eaaa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From the above, we can see a column that is unnamed, along with the case order column.\n",
    "### Both of these columns can be dropped since they are not relevant to the churn rate. \n",
    "### This will reduce our columns from 52 to 50, keeping the 10,000 row count in tact. \n",
    "### The names of the survey response columns need to be addressed as well so that they cab be interpreted.\n",
    "### Some statistics that we can see include: The average monthly charge is $174.08, The average customer \n",
    "### age is 53, The average income is $39,936.76 yearly, and All of the survey responses averaged around a 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab084e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We are missing key numerical data from the statistics, so let us look at the data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc732a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_series().groupby(df.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data type inspection reveals a number of object typed variables that we will need to address.\n",
    "### Next, we will see how many missing values there are currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81cf1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e478f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "### There are a number of missing values from the children, age, income, techie, phone, tech support, \n",
    "### tenure, and badnwidth GB year columns. Each column will be addressed individually. \n",
    "### Let's begin some of the clean-up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping the unnamed column.\n",
    "df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "### Dropping the case order column.\n",
    "df = df.drop(['CaseOrder'], axis = 1)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, let's change the column names for the survey responses using a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbeb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey = {'item1': 'TimelyResponse', \n",
    "          'item2': 'TimelyFixes', \n",
    "          'item3': 'TimelyReplacements', \n",
    "          'item4': 'Reliability', \n",
    "          'item5': 'Options', \n",
    "          'item6': 'RespectfulResponse',\n",
    "          'item7': 'CourteousExchange',\n",
    "          'item8': 'EvidenceOfActiveListening'}\n",
    "df.rename(columns = survey, inplace = True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3710b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, we can make sure that no rows are curently duplicates of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14406a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.loc[df.duplicated()]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f07e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Great news! There are no duplicated rows of data. Next, we need to address the missing values.\n",
    "### Before we use any central tendencies to fill the missing values, we need to first detect outlying values.\n",
    "### Each column (children, age, income, techie, phone, tech support,tenure, and bandwidth GB year) are \n",
    "### individually assessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed284663",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Each numeric column (children, age, income, tenure, and bandwidth GB Year) is viewed in a box plot.\n",
    "df.boxplot(['Children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3109e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e06322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(['Income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(['Tenure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b88925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(['Bandwidth_GB_Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d69478",
   "metadata": {},
   "outputs": [],
   "source": [
    "### When looking at this data, we are looking for extreme outlying values that will throw off the \n",
    "### central tendencies we use to fill in the missing values. From these box plots, we see nothing in age, \n",
    "### tenure or bandwidth GB year columns to worry about. The missing values in these columns can easily be \n",
    "### imputed with a central tendency like the median value. \n",
    "### We can see a few outliers in the children column, and several in the income column. With regards to our\n",
    "### analysis, these variables will not effect the churn rate, have outliers, and have a number of missing\n",
    "### values. Both the children and income columns should be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0977f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping the children column.\n",
    "df = df.drop(['Children'], axis = 1)\n",
    "\n",
    "### Dropping the income column.\n",
    "df = df.drop(['Income'], axis = 1)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preserve a copy of the data frame to this point before imputing values.\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imputing the median value into the missing values for the age column.\n",
    "df2['Age'] = df2['Age'].fillna(df2['Age'].median())\n",
    "df2['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45faf0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The missing values in the tenure and bandwidth columns could be due to the fact that the customner \n",
    "### has recently signed up for the service. The tenure does not begin to count until a month has passed.\n",
    "### We can impute a zero in place of all of these missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Tenure'] = df2['Tenure'].fillna(0)\n",
    "df2['Bandwidth_GB_Year'] = df2['Bandwidth_GB_Year'].fillna(0)\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's address the remaining three columns; techie, phone and tech support. For these we will find the\n",
    "### value that most frequently occurs, and fill the missing values with that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aa9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Techie'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Techie'] = df2['Techie'].fillna('No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Phone'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88af396",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Phone'] = df2['Phone'].fillna('Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8174cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['TechSupport'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb555376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['TechSupport'] = df2['TechSupport'].fillna('No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a9228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We now have a complete and clean data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5edbe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract the clean data before the PCA is done.\n",
    "df2.to_csv('churn_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889cda7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a copy of the existing data frame with just the categorical variables.\n",
    "df_cat = df2[['City', 'County', 'Zip', 'Job', 'Timezone', 'Lat', 'Lng', 'Customer_id', 'Interaction', 'State',\n",
    "      'Gender', 'Churn', 'Techie', 'Contract', 'Port_modem', 'Area', 'Tablet', 'Phone', 'OnlineSecurity',\n",
    "      'Multiple', 'OnlineBackup', 'TechSupport', 'DeviceProtection', 'StreamingTV', 'StreamingMovies', \n",
    "      'PaperlessBilling', 'PaymentMethod', 'Marital', 'InternetService', 'Employment', 'Education']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd322a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reload the clean data, removing all categorical variables.\n",
    "churn = pd.read_csv('churn_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Listing the categorical variables.\n",
    "to_drop = ['City', 'County', 'Zip', 'Job', 'Timezone', 'Lat', 'Lng', 'Customer_id', 'Interaction', 'State',\n",
    "      'Gender', 'Churn', 'Techie', 'Contract', 'Port_modem', 'Area', 'Tablet', 'Phone', 'OnlineSecurity',\n",
    "      'Multiple', 'OnlineBackup', 'TechSupport', 'DeviceProtection', 'StreamingTV', 'StreamingMovies', \n",
    "      'PaperlessBilling', 'PaymentMethod', 'Marital', 'InternetService', 'Education', 'Employment']\n",
    "\n",
    "### Drop the columns leaving only numerical data.\n",
    "churn = churn.drop(columns=to_drop)\n",
    "print(churn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b666df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Because all of the columns have varying range values, we need to normalize the data. A pipeline will \n",
    "### do this task using the StandardScaler command in scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f8367",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's first take a look at the correlation of our data in a heatmap. \n",
    "sns.heatmap(churn.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec9968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### It looks like there is not much correlation currently. Bandwidth, tenure, timely responses and timely \n",
    "### fixes are the highest correlating factors from this graph. Options has the lowest correlating factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The PCA will consist of the following steps: StandardScaler and PCA will create a pipeline, The numerical\n",
    "### data will be fit and transformed, The variance for each principal component will be calculated and plotted\n",
    "### The number of components will be determined by the elbow method, The chosen number of components create \n",
    "### the PCA model and fit the data, The data is split into training and testing sets, Finally classifiers are\n",
    "### applied to the pipeline and tested for prediction accuracy with and without the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4a442",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create pipeline and obtain principal components.\n",
    "### Citation: (Scikit-Learn Documentation, 2022)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe1 = Pipeline([('scaler', StandardScaler()),('reducer',PCA())])\n",
    "pc = pipe1.fit_transform(churn)\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b2b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtain the variances.\n",
    "variances = pipe1.steps[1][1].explained_variance_ratio_*100\n",
    "variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6530d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cumulative sum of variances.\n",
    "np.cumsum(variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f7d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scree plot of the variances.\n",
    "plt.plot(variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From this scree plot, we can see that the first 3 and last 3 components are accounting for the data \n",
    "### variation. From tthis, we should keep 11 components for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14897549",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the training and test data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = df_cat['Churn'].apply(lambda x:1 if x == 'Yes' else 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(churn, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc389fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Showing the variables of the training and test data.\n",
    "print(X_train.head())\n",
    "print(X_test.head())\n",
    "print(y_train.head())\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b66dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic regression and PCA to get the churn prediction accuracy.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for i in range (3,15):\n",
    "    pipe2 = Pipeline([('scaler',StandardScaler()), ('reducer',PCA(n_components=i)), \n",
    "                      ('classifier',LogisticRegression())])\n",
    "    pipe2.fit(X_train,y_train)\n",
    "    print(i, pipe2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f4d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic regression to get churn prediction without PCA.\n",
    "pipe4 = Pipeline([('scaler', StandardScaler()), ('classifier', LogisticRegression())])\n",
    "pipe4.fit(X_train, y_train)\n",
    "pipe4.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d865fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using logistic regression, we are obtaining 82.4% accuracy of prediction from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b1fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random forest classifier with PCA to calculate churn prediction accuracy.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for i in range (3,15):\n",
    "    pipe5 = Pipeline([('scaler',StandardScaler()),\n",
    "                      ('reducer',PCA(n_components=i)),\n",
    "                      ('classifier',RandomForestClassifier())])\n",
    "    #pipe5.fit(X_train, y_train)\n",
    "    X = X_train\n",
    "    y = y_train    \n",
    "    pipe5.fit(X,y)\n",
    "    print(f\"principal component: {i}, {pipe5.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random forest classifier to calculate churn prediction accuracy without PCA.\n",
    "pipe6 = Pipeline([('scaler',StandardScaler()), ('classifier',RandomForestClassifier())])\n",
    "pipe6.fit(X_train, y_train)\n",
    "print(pipe6.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cac9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From the random forest classifier, our model has an accuracy of 83.5%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
